---
title: "Vc_Chromosome_Fusion_Scripts"
output: html_document
date: "2024-05-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
```
# Sequence analysis of isolates collected for this study
## Assembly and Quality control
### Trycycler

As a check to make sure the fused structure is no assembly error, I assemble 10 of the fused ones (one per patient) and three non-fused ones as controls using trycycler following the tutorial. 
I subsample each fastq files 12 times using the following: 

```{bash, echo=T, eval=F}
# filter reads using filtlong
/home/acuenod/software/venvs/Filtlong/bin/filtlong --min_length 1000 --keep_percent 95 "$input_dir"/"$sample".fastq.gz > "$filt_dir"/"$sample".fastq.gz

# subsample reads
trycycler subsample --reads "$filt_dir"/"$sample".fastq.gz --out_dir "$subsample_dir"/"$sample" --count 12 --genome_size 4000000 --threads 8

```

Then to assemble, I use flye, raven, miniasm+minipolish and canu as follows: 

```{bash, echo=T, eval=F}
array=(mock 111Vc05 112Vc01 113Vc01 114Vc03 117Vc01 119Vc01 120Vc01 121Vc01 134Vc05 135Vc02 114Vc01 128Vc06 142Vc02)

## define the variable 'sample_id'from the 'array' defined above
#flye
for i in 01 05 09; do
    /home/acuenod/software/venvs/Flye/bin/flye --nano-hq "$subsample_dir"/sample_"$i".fastq --threads "$threads" --out-dir "$assembly_dir"_"$i" && cp "$assembly_dir"_"$i"/assembly.fasta "$assembly_dir"/assembly_"$i".fasta && cp "$assembly_dir"_"$i"/assembly_graph.gfa "$assembly_dir"/assembly_"$i".gfa && rm -r "$assembly_dir"_"$i"
done

#miniasm + minipolish
for j in 02 06 10; do
    /home/acuenod/software/venvs/Minipolish/miniasm_and_minipolish.sh "$subsample_dir"/sample_"$j".fastq "$threads" > "$assembly_dir"/assembly_"$j".gfa && /home/acuenod/software/venvs/any2fasta/any2fasta "$assembly_dir"/assembly_"$j".gfa > "$assembly_dir"/assembly_"$j".fasta
done

#raven
for k in 03 07 11; do
    raven --threads "$threads" --disable-checkpoints --graphical-fragment-assembly "$assembly_dir"/assembly_"$k".gfa "$subsample_dir"/sample_"$k".fastq > "$assembly_dir"/assembly_"$k".fasta
done

#canu
canu_tmp_dir=/home/acuenod/scratch/household/01_data/18_trycycler/assemblies/canu_temp_"$sample"
for l in 04 08 12; do
    canu -p canu -d "$canu_tmp_dir"_"$l"  -fast genomeSize=4000000 useGrid=false maxThreads="$threads" -nanopore "$subsample_dir"/sample_"$l".fastq
    /home/acuenod/software/venvs/edlib/trycycler/canu_trim.py "$canu_tmp_dir"_"$l"/canu.contigs.fasta > "$assembly_dir"/assembly_"$l".fasta
    rm -rf "$canu_tmp_dir"_"$l"
done

```

Next, I manually checked how many big (>=1MB) contigs were assembles and how many of these were circular. I checked all gfa files in bandage, except for the canu assemblies (here I had no gfa file, but checked the contig headers). for the next, I exclude all for which not all large assemblies were circular (see 01_household_study/01_data/16_check_singular_contig/trycycler/Check_bandage_manually.xlsx)

I moved the ones to be excluded to assemblies_excluded. 

I next clustered the remaining contigs using: 

```{bash, eval=F, echo=T}
trycycler cluster --assemblies "$assembly_dir"/*fasta --reads "$reads" --out_dir "$cluster_dir"
```


Trycycler cluster outputs a tree per sample. I consider all clusters as valuable, if they occure in at least two assemblies and are of similar size. 
134Vc05 could somehow not be opened in figtree, I read the raw newick file instead. On the clusters which were kept, run trycycler reconcile: 

```{bash, echo=T, eval=F}
for cluster in $(ls -d ${cluster_dir}/cluster* |sed 's/.*\///g')
do
  echo ${cluster}
  trycycler reconcile  --reads "$reads"  --cluster_dir "$cluster_dir"/${cluster} --threads "$threads"
done



```


Some contigs caused the following error: 
"Error: some pairwise worst-1kbp identities are below the minimum allowed value
of 25.0%. Please remove offending sequences or lower the --min_1kbp_identity
threshold and try again.". --> I excluded the sequences causing the error. 

Some failed to circularise, I exclude these too. 
Then run tryclcler reconcile again. 

Then, run trycycler msa and partition the reads with trycycler partition. trycycler msa:

```{bash, echo=T, eval=F}
for cluster in $(ls -d ${cluster_dir}/cluster* |sed 's/.*\///g')
do
  echo ${cluster}
  trycycler msa --cluster_dir "$cluster_dir"/${cluster} --threads "$threads"
done
```
 
 Then trycycler partition:
```{bash, echo=T, eval=F}
reads=/lustre04/scratch/acuenod/household/01_data/18_trycycler/reads/1_filtered/"$sample".fastq.gz
cluster_dir=/lustre04/scratch/acuenod/household/01_data/18_trycycler/clusters/"$sample"

mkdir "$cluster_dir"

trycycler partition --reads "$reads" --cluster_dirs "$cluster_dir"/cluster_* --threads "$threads"
```

Then build a consensus: 
```{bash, echo=T, eval=F}
reads=/lustre04/scratch/acuenod/household/01_data/18_trycycler/reads/1_filtered/"$sample".fastq.gz
cluster_dir=/lustre04/scratch/acuenod/household/01_data/18_trycycler/clusters/"$sample"

for cluster in $(ls -d ${cluster_dir}/cluster* |sed 's/.*\///g')
do
  echo ${cluster}
  trycycler consensus --cluster_dir "$cluster_dir"/${cluster} --threads "$threads"
done


```

And summarise the consensus

```{bash, echo=T}

for sample in $(ls -d /lustre04/scratch/acuenod/household/01_data/18_trycycler/clusters/1*)
do
  echo ${sample}
  cat ${sample}/cluster_*/7_final_consensus.fasta > ${sample}/consensus.fasta
done

```

### Quality control
The trycycler assemblies looked similar (especially in termsof fusion / non-fusion) than the assemblies generated by flye (not on subset, but on all reads), which I used for the rest of the analysis. 
I used the following for quality control of the flye assemblies: 
  - To get stats on the assemblies use Nanostats and Quast
  - remap all reads to the generated assembly. Use the resulting bam file to calculate the overall read depth 
  - summarise all to one table

```{bash, echo=T, eval=F}

# This script has been adapted from a script originally written by Dr. Daniel WÃ¼thrich in the research group of Prof. Adrian Egli (aegli@imm.uzh.ch) 

# assign reads to new variable
R=00_reads/fastq/"$sample_id".fastq.gz

# assign assembly to variable
assembly=01_assemblies/"$sample_id".fasta

mkdir -p 02_quality/"$sample_id"

# get read stats using nanostat
source "$venv_dir"/nanostats/bin/activate
NanoStat -t "$SLURM_CPUS_PER_TASK" --fastq "$R" --outdir 02_quality/"$sample_id"/nanostat/ --name "$sample_id" --tsv
deactivate

##get assembly stats using quast
module load StdEnv/2020  gcc/9.3.0 quast/5.0.2
quast --min-contig 0 -o 02_quality/"$sample_id"/quast/ "$assembly"
module unload quast/5.0.2

#remap the reads to the assembly (using minimap) to calculate read depth
# load modules
module load bwa/0.7.17
module load samtools/1.17
module load minimap2/2.24
mkdir -p 02_quality/"$sample_id"/remapping/mapping/
cp "$assembly" 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna
# index assembly
bwa index 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna
# map
minimap2 -ax map-ont -t "$SLURM_CPUS_PER_TASK" 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna "$R"  > 02_quality/"$sample_id"/remapping/mapping/alingnment.sam
# sort the sam file and convert to bam
samtools sort -@ "$SLURM_CPUS_PER_TASK" -T 02_quality/"$sample_id"/remapping/mapping/temp_sort -o 02_quality/"$sample_id"/remapping/mapping/alingnment.bam 02_quality/"$sample_id"/remapping/mapping/alingnment.sam
# remove duplicates (not sure?) and index
samtools rmdup 02_quality/"$sample_id"/remapping/mapping/alingnment.bam 02_quality/"$sample_id"/remapping/mapping/alingnment.removed_duplicates.bam
samtools index 02_quality/"$sample_id"/remapping/mapping/alingnment.removed_duplicates.bam
# calculate read depth
samtools depth -aa 02_quality/"$sample_id"/remapping/mapping/alingnment.removed_duplicates.bam | awk '{sum+=$3;count+=1;}END{print sum/count;}'  > 02_quality/"$sample_id"/remapping/coverage.tab
module unload bwa/0.7.17
module unload samtools/1.17
rm 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna

## summarise quality stats
mkdir 02_quality/"$sample_id"/summary
## fetch median read length, n50 and median read quality from nanostats
awk '$1 == "median_read_length" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1a.txt
awk '$1 == "n50" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1b.txt
awk '$1 == "median_qual" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1c.txt
awk '$1 == "mean_qual" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1d.txt
awk '$1 == "ALL" {print $8}' 02_quality/"$sample_id"/seqtk_fqchk/"$sample_id".tab > 02_quality/"$sample_id"/summary/1e.txt
# fetch the read depth from the remapping
cp 02_quality/"$sample_id"/remapping/coverage.tab 02_quality/"$sample_id"/summary/2.txt
# fetch coontig count from quast
tail -n 1 02_quality/"$sample_id"/quast/transposed_report.tsv | awk '{print $14 "\t" $16  "\t" $18  "\t" $17 }' > 02_quality/"$sample_id"/summary/3.txt
# summaris quality stats for all samples
paste <(echo "$sample_id") 02_quality/"$sample_id"/summary/*.txt > 02_quality/"$sample_id"/summary/"$sample_id".tab
echo -e "Sample\tMedian_read_length\tN50_reads\tMedian_read_quality_nanostat\tMean_read_quality_nanostat\tMean_read_quality_seqtk\tRead_depth\tContig_count\tTotal_length_assembly\tN50_assembly\tGC_percent" >  02_quality/quality_plate2_4.tab
cat 02_quality/*/summary/*.tab >> 02_quality/quality.tab
# count the number of samples analysed
let sample_count=$(grep -c "" 02_quality/quality.tab)-1
echo "analysed_samples: $sample_count" >> 02_quality/quality.tab
# convert tab to csv for export
sed 's/,/;/' 02_quality/quality.tab | sed 's/\t/,/g' > 02_quality/quality.csv

```

To retrieve information on circularity of the contigs, I summarised all info files from the flye assemblies using the following:

```{bash, eval=FALSE, echo=TRUE}

for sample in $(cat /lustre04/scratch/acuenod/household/01_data/samples.txt); do   
  echo "${sample}"
  infile=/lustre04/scratch/acuenod/household/01_data/999_raw/AW02/assembliesFlye/"${sample}"/assembly_info.txt
  sed "s/$/&\t${sample}/" "$infile" >> all_fly_assembly_info.txt
done
#remove all header lines but first one 
sed -i -e '/^\#seq\_name/{x;/./!{x;h;b;};d}' all_fly_assembly_info.txt
```

Evaluate this in plot_QC_ONT.R

### Annotation
Each sample was annotated using bakta: 
```{bash, echo=T, eval=F}
bakta  --verbose --keep-contig-headers --db "$db" "$input_assembly" --output "$outbut_dir" --force
```


## Phylogeny
### SNV tree
To generate a SNV based phylogenetic tree, I did the following: 
- map reads to an external reference (Vc N16961) to call SNV using medaka
- Filter out indels
- extract low quality (lower than Qscore 40, corresponds to 0.0001 error probability) - from VCF to bed
- use bcftools consensus to build consensus and mask these low quality sites (I do this per sample)
- concatenate
- build tree using raxml --> iqtree

Medaka: 
```{bash, echo=T, eal=F}
ml apptainer
## define read directory
read_dir=/lustre04/scratch/acuenod/household/01_data/00_reads/fastq
ref=/lustre04/scratch/acuenod/household/01_data/11_tree/ref/Vc_N16961/GCF_001250235.2_5174_7_8_v2_genomic.fna
outdir=/lustre04/scratch/acuenod/household/01_data/11_tree/output/ref_N16961

apptainer run -e -B /lustre04/scratch/ /home/acuenod/software/containers/medaka.sif medaka_haploid_variant -x  -i "$read_dir"/"$sample_id".fastq.gz -r "$ref" -o "$outdir"/"$sample_id" -f -t "$threads" -m r1041_e82_400bps_sup_variant_v4.2.0
```

Three files failed repeatadly at the annotation step (120Vc05 130Vc10 145Vc02). (I tried increasing memory (although they are not bigger than other files), but the error remains). 
I think I do not need the annotation, as I continue with the produced medaka.vcf file which is produced for these without error. Next, I filter these .vcf files (remove indels and build consensus with lowQ SNV masked)

```{bash, echo=T, eval=F}
# remove indels using vcf tools # ' --recode --recode-INFO-all' is needed, no output is produced otherwise
ml  StdEnv/2023 vcftools/0.1.16
vcftools --vcf  "$vcf_raw" --remove-indels --recode --recode-INFO-all --out "$vcf_no_indels"

ml StdEnv/2020 bedops/2.4.41
# use bedops vcf2bed to get all lowQ (<40 as this is equal to a error probability of 0.0001) variants to mask
vcf2bed --keep-header < "$vcf_no_indels".recode.vcf | awk '{if($5 < 40) print}'  > "$lowQ_sites_bed"

ml StdEnv/2023  gcc/12.3 bcftools/1.18
bgzip -c "$vcf_no_indels".recode.vcf > "$vcf_no_indels".gz
bcftools index "$vcf_no_indels".gz

bcftools consensus --prefix "$sample" --fasta-ref "$ref_fasta" --mask "$lowQ_sites_bed"  --output "$hq_snv" "$vcf_no_indels".gz
```

Then to concatenate the consensus per sample using: 
```{bash, echo=T, eval=F}
echo ">"$sample"" > /lustre04/scratch/acuenod/household/01_data/11_tree/output/ref_N16961/"$sample"/"$sample"_highQ_variants_no_indels_concat.fasta
grep -v '>' /lustre04/scratch/acuenod/household/01_data/11_tree/output/ref_N16961/"$sample"/"$sample"_highQ_variants_no_indels.fasta >> /lustre04/scratch/acuenod/household/01_data/11_tree/output/ref_N16961/"$sample"/"$sample"_highQ_variants_no_indels_concat.fasta

```

Tree is compiled using RAxML: 
```{bash, echo=T, eval=F}
raxmlHPC -T "$SLURM_CPUS_PER_TASK" -x 1522 -f a -m GTRCAT -p 1522 -# 10 -s /lustre04/scratch/acuenod/household/01_data/11_tree/output/all_concat_highQ_variants_no_indels_concat.fasta -w /lustre04/scratch/acuenod/household/01_data/11_tree/output/raxml/ -n raxmltree_all -V AVX2

```

Repeat IQtree with bootstraps on SP (I realise RaXML is not ideal for this dataset, as it assigned minimal bramch lengths, even for the sam sequences)
```{bash}
source ~/miniconda3/bin/activate
cd /mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/05_tree/iqtree
input=input/all_concat_highQ_variants_no_indels_concat_snp-sites.fasta

iqtree2 -s "$input" --ufboot 5000 --boot-trees -T 128

```


### Lineage Assignment
To have a solid way to call lineages within pandemic Vc strains, I chose one reference strain per lineage (see 'lineage_assignmnet.R'), downloaded the reads and assembled these (as usual). 
I then calculated fastANI of my strains to all of these. Unfortunately, this was not unambiguous, as sometimes I see very high values to more than one reference strain. As this is based on the assembly, sequencing errors can play a role here. To circumvent this I now used medaka and filtered for high quality SNV of all my strains against the references using the following: 

```{bash, echo=T, eval=F}
ml apptainer
## define read directory
read_dir=/lustre04/scratch/acuenod/household/01_data/00_reads/fastq
ref_dir=/lustre04/scratch/acuenod/household/01_data/17_lineage_assignment/refs/fasta/
outdir=/lustre04/scratch/acuenod/household/01_data/17_lineage_assignment/medaka/out/

ml StdEnv/2023 vcftools/0.1.16

for ref in $(ls "$ref_dir")
do
  apptainer run -e -B /lustre04/scratch/ /home/acuenod/software/containers/medaka.sif medaka_haploid_variant -i "$read_dir"/"$sample_id".fastq.gz -r "$ref_dir"/"$ref" -o "$outdir"/"$sample_id"/"$ref" -f -t "$threads" -m r1041_e82_400bps_sup_variant_v4.2.0
  vcf_raw="$outdir"/"$sample_id"/"$ref"/medaka.vcf
  vcf_hqsnv="$outdir"/"$sample_id"/"$sample_id"_"$ref"_medaka_hq_snv.vcf
  tab_hqsnv="$outdir"/"$sample_id"/"$sample_id"_"$ref"_medaka_hq_snv.tab
  # there is no depth info, and I know that real SNV are usually very high quality, therefore, in crease minQ, leave all other
  # quality 40 corresponds to 0.0001 error probability
vcftools --vcf "$vcf_raw"  --minQ 40 --recode --recode-INFO-all --out "$vcf_hqsnv"
done

```

To summarise these and count hq SNV I used the following: 

```{bash, echo=T, eval=F}
cd /home/acuenod/scratch/household/01_data/17_lineage_assignment/medaka/out
grep -c -v '^\#' */*_medaka_hq_snv.vcf.recode.vcf > hq_snv_counts.txt
```
Evaluate in 'lineage_assignment.R'
## HS1
### Identify and screen HS1
of 117Vc07 and 134Vc04 I blasted chromosome 1 against chromosome 2 using the following: 

```{bash}
makeblastdb -in /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast/input/db/134Vc041.fasta -dbtype nucl -out /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast/input/db/134Vc041

db=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast/input/db/134Vc041
query=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast/input/query/134Vc042.fasta
out=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast/output/134Vc042_vs_134Vc041.tab

blastn -query "$query" -db  "$db" \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send qseq sseq' \
  -max_target_seqs 5000 -out "$out"

```

--> use the overlav of 134Vc04 chr1 and 134Vc04 chr 2 as query to screen other the same sequence in other genomes using blastn:

```{bash, echo=TRUE, eval=FALSE}
# load module
ml StdEnv/2020 gcc/9.3.0 blast+/2.13.0

db=/lustre04/scratch/acuenod/household/01_data/05_blast/db/householddb
query=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast_overlap_against_all/input/query/overlap_134Vc041_134Vc042.fasta
out=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/find_homologuous_region_between_chr/blast_overlap_against_all/output/blastout_HR_all.tab

blastn -query "$query" -db  "$db" \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send' \
  -max_target_seqs 5000 -out "$out"
```
Then also evaluated in 'eval_read_count_HR.R'. 

### annotate HS1 using bakta

```{bash}
input_assembly=/lustre04/scratch/acuenod/household/01_data/03_bakta/12kb_overlap/overlap_134Vc041_134Vc042.fasta
outbut_dir=/lustre04/scratch/acuenod/household/01_data/03_bakta/12kb_overlap/overlap

## added '--keep-contig-headers' flag, rerun with this when adding new sequences
bakta  --verbose --keep-contig-headers --db /lustre03/project/6077363/GROUP/01_db/bakta/ "$input_assembly" --output "$outbut_dir" --force

```

### check orientation of HS1 by annotation
To subset the bakta annotation for exeA, do the, use grep: 
```{bash, echoTT, eval=F}
# from within the bakta folder do: 
grep "Type II secretory pathway ATPase component GspA" */*.tsv > check_HS1_exeA_public_fused.tsv # for the public fused Vc and 
grep "Type II secretory pathway ATPase component GspA" */*.tsv > check_HS1_exeA.tsv # for all our genomes
```
evaluate in 'eval_read_count_HR.R' and 'public_long_reads.R' respectively.  

### Count reads spanning HS1 in the fused and non-fused state
I realised that when checking how many reads span a region (the HR region on the fused chromosomes (aka junctions) or the HR regions on the non-fused ones) I need to exclude reads which have a large deteletion compared to the reference. (I realised as I had read ID in my outputs from BD1.2 strains chr2 which do not contain the 12kb HR, the reads still mapped, but with a large deteltion, these need to be removed). 


For the junctions (chr1-HS1-chr2 or chr2-HS1-chr1, for the two junctions) I can the following: 
```{bash, echo=T, eval=F}
#load apptainer module 
ml  StdEnv/2020 samtools/1.17

input=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/read_depth_2chr_at_junction/readmapping/"$sample".sorted.bam
input_bed_junction=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/135Vc06_junctions_extended_HR_13087.bed #all were mapped agains 135Vc06 (junction position always stays the same)
output_bed=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/beds_out/"$sample".bed
output_bed_no_inserts=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/beds_out/"$sample"_no_inserts_larger_1000.bed
output_readID_junction_1=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction1.txt
output_readID_junction_2=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction2.txt


# bam to bed
ml bedtools/2.29.2
# convert bam to bed and filter bed for reads. remove duplicated reads and non-primary alignments
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |bedtools bamtobed > "$output_bed"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}\D)/' |bedtools bamtobed > "$output_bed_no_inserts"

#bedtools bamtobed -i "$input" > "$output_bed"
# extract reads spanning junction 1
junction_1_1=$(awk '$4== "junction_1" {print $2}' "$input_bed_junction")
junction_1_2=$(awk '$4== "junction_1" {print $3}' "$input_bed_junction")
#bedtools bamtobed -i "$input" | awk -v var1="$junction_1_1" -v var2="$junction_1_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_1":"junction_1")} 1' > "$output_readID_junction_1"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP | awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' | bedtools bamtobed | awk -v var1="$junction_1_1" -v var2="$junction_1_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_1":"junction_1")} 1' > "$output_readID_junction_1"

# extract reads spanning junction 2
junction_2_1=$(awk '$4== "junction_2" {print $2}' "$input_bed_junction")
junction_2_2=$(awk '$4== "junction_2" {print $3}' "$input_bed_junction")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP | awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' | bedtools bamtobed | awk -v var1="$junction_2_1" -v var2="$junction_2_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_2":"junction_2")} 1' > "$output_readID_junction_2"


```

and for the HS1 on the chromosome (chr1 OR chr2) the following: 
```{bash, echo=T, eval=F}
#load apptainer module 
ml  StdEnv/2020 samtools/1.17

input=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/read_depth_2chr_at_junction/readmapping_non-fused_HR/"$sample".sorted.bam
input_bed_HR_chr=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused/135Vc05_junctions_extended_HR_13087.bed #all were mapped agains 135Vc06 (HR_chr position always stays the same)
output_bed=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused/beds_out/"$sample".bed
output_bed_no_inserts=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused/beds_out/"$sample"_no_inserts_larger_1000.bed
output_readID_HR_chr_1=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused/out/"$sample"_reads_spanning_HR1.txt
output_readID_HR_chr_2=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused/out/"$sample"_reads_spanning_HR2.txt


# bam to bed
ml bedtools/2.29.2
# convert bam to bed and filter bed for reads. remove duplicated reads and non-primary alignments
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |bedtools bamtobed > "$output_bed"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}\D)/' |bedtools bamtobed > "$output_bed_no_inserts"
# this second line filters reads out which have an deletion (when compared to the reference of 1000 or more. (this first awk expression extracts the headers, the second excludes reads which have a 4-digit-D pattern in their CIGAR string)

#bedtools bamtobed -i "$input" > "$output_bed"

# extract reads spanning HR_chr 1
HR_chr_1_1=$(awk '$4== "HR_chr_1" {print $2}' "$input_bed_HR_chr")
HR_chr_1_2=$(awk '$4== "HR_chr_1" {print $3}' "$input_bed_HR_chr")
chr1_name=$(awk '$4== "HR_chr_1" {print $1}' "$input_bed_HR_chr")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' |bedtools bamtobed | awk -v var0="$chr1_name" -v var1="$HR_chr_1_1" -v var2="$HR_chr_1_2" '{ if ($1 == var0 && $2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"HR_chr_1":"HR_chr_1")} 1' > "$output_readID_HR_chr_1"

# extract reads spanning HR_chr 1
HR_chr_2_1=$(awk '$4== "HR_chr_2" {print $2}' "$input_bed_HR_chr")
HR_chr_2_2=$(awk '$4== "HR_chr_2" {print $3}' "$input_bed_HR_chr")
chr2_name=$(awk '$4== "HR_chr_2" {print $1}' "$input_bed_HR_chr")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' |bedtools bamtobed | awk -v var0="$chr2_name" -v var1="$HR_chr_2_1" -v var2="$HR_chr_2_2" '{ if ($1 == var0 && $2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"HR_chr_2":"HR_chr_2")} 1' > "$output_readID_HR_chr_2"

```

using the file ID I extracted and converted the respective reads to fasta files.
for the junctions: 
```{bash, echo=T, eval=F}

input_fastq=/home/acuenod/scratch/household/01_data/00_reads/fastq/"$sample".fastq.gz
# define variables for junction1
read_id_file_junction1=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction1.txt
awk -F' ' '{print $1}' "$read_id_file_junction1" > /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction1_names_tmp.txt
read_id_junction1=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction1_names_tmp.txt
output_fasta_junction1=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/fasta_from_reads/"$sample"_junction1.fasta

# define variables for junction2
read_id_file_junction2=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction2.txt
awk -F' ' '{print $1}' "$read_id_file_junction2" > /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction2_names_tmp.txt
read_id_junction2=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction2_names_tmp.txt
output_fasta_junction2=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/fasta_from_reads/"$sample"_junction2.fasta

# load packages
ml nixpkgs/16.09  gcc/7.3.0 seqtk/1.3
# extract reads spanning junction1
seqtk subseq "$input_fastq" "$read_id_junction1" | seqtk seq -a - > "$output_fasta_junction1"
# extract reads spanning junction2
seqtk subseq "$input_fastq" "$read_id_junction2" | seqtk seq -a - > "$output_fasta_junction2"

rm "$read_id_junction1"
rm "$read_id_junction2"

```

For the HR region on the chromosome: 
```{bash, echo=T, eval=F}
#load apptainer module 
ml  StdEnv/2020 samtools/1.17

input=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/read_depth_2chr_at_junction/readmapping/"$sample".sorted.bam
input_bed_junction=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/135Vc06_junctions_extended_HR_13087.bed #all were mapped agains 135Vc06 (junction position always stays the same)
output_bed=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/beds_out/"$sample".bed
output_bed_no_inserts=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/beds_out/"$sample"_no_inserts_larger_1000.bed
output_readID_junction_1=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction1.txt
output_readID_junction_2=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/out/"$sample"_reads_spanning_junction2.txt


# bam to bed
ml bedtools/2.29.2
# convert bam to bed and filter bed for reads. remove duplicated reads and non-primary alignments
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |bedtools bamtobed > "$output_bed"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}\D)/' |bedtools bamtobed > "$output_bed_no_inserts"

#bedtools bamtobed -i "$input" > "$output_bed"
# extract reads spanning junction 1
junction_1_1=$(awk '$4== "junction_1" {print $2}' "$input_bed_junction")
junction_1_2=$(awk '$4== "junction_1" {print $3}' "$input_bed_junction")
#bedtools bamtobed -i "$input" | awk -v var1="$junction_1_1" -v var2="$junction_1_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_1":"junction_1")} 1' > "$output_readID_junction_1"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP | awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' | bedtools bamtobed | awk -v var1="$junction_1_1" -v var2="$junction_1_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_1":"junction_1")} 1' > "$output_readID_junction_1"

# extract reads spanning junction 1
junction_2_1=$(awk '$4== "junction_2" {print $2}' "$input_bed_junction")
junction_2_2=$(awk '$4== "junction_2" {print $3}' "$input_bed_junction")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP | awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' | bedtools bamtobed | awk -v var1="$junction_2_1" -v var2="$junction_2_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_2":"junction_2")} 1' > "$output_readID_junction_2"

```

To check if these are genuin read or of these might be hybrid reads (then thy would have an adapter sequence where merged), I blasted the adapter sequences agains these fastas: 

```{bash, echo=T, eval=F}

db_dir=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/spanning_junction_adapter/fasta_from_reads/
query_dir=/home/acuenod/scratch/household/01_data/13_minimap_alignmnets/spanning_junction_adapter/adapters/
out_dir=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/spanning_junction_adapter/blastn_adapters_out/

cat "$db_dir"/*.f* > "$db_dir"/reads_spanning_junction.fasta
makeblastdb -in "$db_dir"/reads_spanning_junction.fasta -dbtype nucl -out "$db_dir"/reads_spanning_junction

# run blast
for file in $(ls "$query_dir"/*.fasta)
do
  query_name_temp="${file/%.f*/}"
  query_name="${query_name_temp/*\//}"
  echo "${query_name}"
#  blastn -query "${file}" -db "$db_dir"/reads_mapping_3rd_contig \
#  blastn -query "${file}" -db "$db_dir"/short_contig \
  blastn -query "${file}" -db "$db_dir"/reads_spanning_junction \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend qseq slen sstart send sstrand sseq' \
  -max_target_seqs 100000 -out "$out_dir"/"${query_name}".tab 
#   -max_target_seqs 1000 -out "$out_dir"/"${query_name}".tab
done
```

Eval in 'eval_read_count_HR.R'. I export all read ID in which an adapter was found at least 1 time. 
I then remove the read ID of reads with adapter and count again: 

```{bash, echo=T, eval=F}
#for the non-fused HR
cd /home/acuenod/scratch/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused

for file in $(ls out/1*txt|sed 's/out\///g')
do
  echo "$file"
  grep -vf  /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_adapter_found.txt out/"$file" > out_exclude_reads_w_adapters/"$file"
done

cd out_exclude_reads_w_adapters
wc -l * >  read_count_spanning_HR_non-fused_adapter_excl.txt

# for the junctions
cd /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR

for file in $(ls out/1*txt|sed 's/out\///g')
do
  echo "$file"
  grep -vf  /lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_adapter_found.txt out/"$file" > out_exclude_reads_w_adapters/"$file"
done

cd out_exclude_reads_w_adapters
wc -l * >  read_count_spanning_HR_junction_adapter_excl.txt


```

## Pathogenicity islands: 
Flanking sides:
GCF_003097695.1 was downloaded today from NCBI Refseq. This is Vibrio cholerae strain A1552 including the annotation used in the JaskÃ³lska 2022 paper. 
- These are the pathogenicity islands I would like to extract.  
        - VPI-1: VC0817-VC0847 
        The gene numbers for VPI-1 come from here
        (https://journals.asm.org/doi/10.1128/jb.00704-15?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub++0pubmed),
        I am not 100% sure if this is correct. same gene map ID numbers used
        here: https://www.frontiersin.org/files/Articles/561296/fcimb-10-561296-HTML-r1/image_m/fcimb-10-561296-t002.jpg
        and fits the gene numbering of this genome. Found more refs, its the
        same numberin everywhere. (Gene map ID?) 
        - VPI-2: VC1757-VC181 (ddmDE: VC1771-VC1770)
        - VSP-I: VC0174-VC0186 (Numbering comes from here: https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1010250)
        - VSP-II: VC0489-VC0517 (ddmABC: VC0492-VC0490)
--> extract the flanking regions of these from the reference genome and blast
these. --> get location of islands and extract these. 

I blasted these flanking sides against all. For one genome (114Vc01) I created bed files of the pathogenicity island location (between flaking sides) and extracted them using the following: 

```{bash, echo=T, eval=F}
ml bedtools/2.29.2
input_fasta=/home/acuenod/scratch/household/01_data/01_assemblies/114Vc01.fasta

input_bed=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VSP-I_114Vc01.bed
output=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VSP-I_114Vc01.fasta
# Include -s flag, forces strandedness. If the feature occupies the antisense strand, the sequence will be reverse complemented
bedtools getfasta -fi "$input_fasta" -bed "$input_bed" -fo "$output" -s

input_bed=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VSP-II_114Vc01.bed
output=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VSP-II_114Vc01.fasta
bedtools getfasta -fi "$input_fasta" -bed "$input_bed" -fo "$output" -s

input_bed=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VPI-1_114Vc01.bed
output=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VPI-1_114Vc01.fasta
bedtools getfasta -fi "$input_fasta" -bed "$input_bed" -fo "$output" -s


input_bed=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VPI-2_114Vc01.bed
output=/lustre04/scratch/acuenod/household/01_data/05_blast/output/VP/VPI-2_114Vc01.fasta
bedtools getfasta -fi "$input_fasta" -bed "$input_bed" -fo "$output" -s
```

I could then use the outputs to screen these in all genomes using blastn. 

## dam sequence comparison

### 2024-05-29
This also matches the reference Uniprot reference: 
>sp|A5F520|DMA_VIBC3 DNA adenine methylase OS=Vibrio cholerae serotype O1 (strain ATCC 39541 / Classical Ogawa 395 / O395) OX=345073 GN=dam PE=3 SV=1
MKKQRAFLKWAGGKYSLVEDIQRHLPEARELVEPFVGAGSVFLNTDFERYLLADINPDLI
NFYNLLKTEPQAYIHEAKRWFVPENNRKEVYLDIRKQFNQSDDAMFRSLAFLYMNRFGFN
GLCRYNKKGGFNVPFGSYKKPYFPEQELEFFAEKAQRATFICASYGETFARAQSDSVIYC
DPPYAPLSTTANFTSYAGNGFTLDDQAALADIAEKTAKERGISVLISNHDTTHTRRLYRG
AQLNVVKANRTISRNGAGRNKVDELLALFTPHLSSQA

### 2023-12-11
It has been shown in the lab, that when dam is mutated (needed for chr. 2 replication), chr 2 fuses with chr. 1 to survive (https://onlinelibrary.wiley.com/doi/epdf/10.1111/mmi.12483). 
Therefore, check the dam sequences. I see from the bakta tsv file, that 'dam' genes are called 'DNA adenine methylase'. extract the protein sequences using seqkit: 

```{bash, echo=TRUE, eval=FALSE}
ml StdEnv/2023 seqkit/2.5.1
seqkit grep -nrf dam_id_grep.txt faa/*faa > dam_aa_seq.faa # where dam_id_grep.txt contains one line 'DNA adenine methylase'

```
evaluated in 'eval_read_count_HR.R' --> all 467 have the exact same dam aa sequence. 


## compare oriC and ctrS
In this paper(https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2018.02932/full)  they describe two naturally fused V. cholerae  strains, in which either both origins of replication are active, or only one, depending on the location crtS.
Therefore, check the copy number variation of crtS. 

To check whether both oris were found in the fused chromosomes:  
```{bash, echo=T, eval=F}
grep "oriC" */*.tsv > oriC.tsv
```

To find crtS in my genomes I use the crtS sequence described by Bruhn et al. (see ref above) for NSCV1

>crtS_NSCV1
AAAATTGAAATGCAGAATATGTAACTTATGCTTTCGGTGGGTGGTTATATAAAAGTTCTTTTCATGGCAGATCGCTGATCAA

and blastn to screen this in all genomes:

```{bash, echo=T, eval=F}
# load module
ml StdEnv/2020 gcc/9.3.0 blast+/2.13.0

db=/lustre04/scratch/acuenod/household/01_data/05_blast/db/householddb
query=/lustre04/scratch/acuenod/household/01_data/05_blast/seqs_to_query/crtS_site.fasta
out=/lustre04/scratch/acuenod/household/01_data/05_blast/output/blastout_crtS_site_household_genomes.tab

blastn -task blastn-short -query "$query" -db  "$db" \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send' \
  -max_target_seqs 5000 -out "$out"

```

Then evaluate both in 'eval_read_count_HR.R'

## par genes
par genes play a role in partitioning the chromosomes to each cell after replication https://pubmed.ncbi.nlm.nih.gov/17197419/
when they are depleted, chr2 is lost. 
I therefore want to check of these are lost in our fused chromosome. 
I first downloaded their sequences from NCBI (searching ((partitioning protein parB) AND vibrio cholerae[Organism])) in Genes and blast them in our genomes. These are each only found once per genome and only on chr1. (I therefore think that these are parAB1 and that these are too far to also fetch parAB2). 
I check the annotation of par genes using 'grep "partitioning protein Par" */*tsv > par_genes_annotated.tsv', to try to find parAB1 (encoded on chr1) and parAB2 (encoded on chr2). none was annotated as parA1 / parB2. I found the proteins sequences for these and from parA and parB on Uniprot / NCBI and used tblastn to screen these in my genomes using tblastn:

```{bash, echo=F, eval=F}

db=/lustre04/scratch/acuenod/household/01_data/05_blast/db/householddb
query_dir=/home/acuenod/scratch/household/01_data/05_blast/seqs_to_query/par_genes/aa
out_dir=/lustre04/scratch/acuenod/household/01_data/05_blast/output/

for file in $(ls "$query_dir")
do
  query_name_temp="${file/%.fasta/}"
  query_name_temp="${query_name_temp/*\//}"
  query_name="$query_name_temp"
  #echo "${file}"
  echo "${query_name}"
  tblastn -query "$query_dir"/"${file}" -db  "$db" \
    -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send sseq' \
    -max_target_seqs 5000 -out "$out_dir"/"${query_name}".tab
done

```

Then evaluate output in 'eval_real_count_HR.R'. 

## rec / mut genes
I wan to check whether there are mutations in RecA. 

```{bash, echo=F, eval=F}
#to get all RecA ID i multifasta do: 
cd /home/acuenod/scratch/household/01_data/03_bakta
grep "recombinase RecA" */*.faa > RecA_ID.txt

#remove all before '>'
sed 's/^.*>//' RecA_ID.txt > RecA_ID_.txt 

awk -F'>' 'NR==FNR{ids[$0]; next} NF>1{f=($2 in ids)} f' RecA_ID_.txt  */*.faa > RecA_seqs.faa

```

I will more systematically check all the mut and all the rec genes
```{bash}
#cd /home/acuenod/scratch/household/01_data/03_bakta/tsv_passaging_ancestors
#awk '$7 ~ /mut|rec/ { print }' *.tsv   > mut_rec_genes.tsv
#awk '$7 ~ /mut|rec/ { print }' *.tsv | awk '{print $6}'  > geneID_mut_rec_genes.tsv ## this gives me all the gene ID, 
## 
# repeat on all
cd /home/acuenod/scratch/household/01_data/03_bakta/
awk '$7 ~ /mut|rec/ { print }' *Vc*/*.tsv   > mut_rec_genes.tsv
awk '$7 ~ /mut|rec/ { print }' *vc*/*.tsv | awk '{print $6}'  > geneID_mut_rec_genes.tsv ## this gives me all the gene ID, 


```
```{r}
#briefly check which which rec / mut genes are found how many times
```

In another appraoch I will check if thee are snv in these genes detected with medaka when mapping against N16961 Ref
To run snpEff on these, I first need to annotate the ref

```{bash}
# bakta

# snpEff
cd /home/acuenod/scratch/household/01_data/11_tree/ref/Vc_N16961
mkdir snpEff
cp /cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/snpeff/5.0/snpEff.config snpEff/
vim snpEff/snpEff.config

# add the following two lines in the db section (under line 130)
#my Vc genome Vc_N16961
Vc_N16961.genome : Vc_N16961

cd snpEff
mkdir -p ./data/Vc_N16961
cp ../GCF_001250235.2_5174_7_8_v2_genomic_concat.fna data/Vc_N16961/sequences.fa
cp ../bakta/GCF_001250235.2_5174_7_8_v2_genomic_concat.gff3 data/Vc_N16961/genes.gff
#in interactive node
ml StdEnv/2020 snpeff/5.0
java -Xmx8g -jar $EBROOTSNPEFF/snpEff.jar build -c snpEff.config -gff3 -v Vc_N16961 > snpEff.stdout

# to extract the reference gene ID of interest (all rec and all mut genes)
/home/acuenod/scratch/household/01_data/11_tree/ref/Vc_N16961/bakta
awk '$7 ~ /mut|rec/ { print }' GCF_001250235.2_5174_7_8_v2_genomic_concat.tsv   > mut_rec_genes.tsv
awk '$7 ~ /mut|rec/ { print }' GCF_001250235.2_5174_7_8_v2_genomic_concat.tsv | awk '{print $6}'  > geneID_mut_rec_genes.tsv ## this gives me all the gene ID

# use geneid to grep in annotated vcf files
cd /home/acuenod/scratch/household/01_data/11_tree/ref/Vc_N16961/snpEff/out
geneID=/home/acuenod/scratch/household/01_data/11_tree/ref/Vc_N16961/bakta/geneID_mut_rec_genes.tsv
for gene in $(cat ${geneID}); do
 echo "$gene"
 grep "$gene" *.annotated.vcf >> snv_in_mut_rec.tab; 
done

# if I run this is a slurm script it gives me a slurm error, but the output looks fine and there is no error when I run this in an interactive node, so I think this is fine
```
--> eval in 'mut_rec.R'


## comparison HR mechanisms to previsously described
To compare whether its the same genes involved in HR for fusion in our strains than described by Xie et al (doi: 10.1155/2017/8724304), I first blasted HS1 against their described fused strains: 

```{bash}
db=/home/acuenod/scratch/household/01_data/16_public_seqs/NSCV1_NSCV2/NSCV1-2
query=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/blast/input/query/overlap_134Vc041_134Vc042.fasta
out=/home/acuenod/scratch/household/01_data/16_public_seqs/NSCV1_NSCV2/blastout_HS1_NSCV1-2.tab

blastn -query "$query" -db  "$db" \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send' \
  -max_target_seqs 5000 -out "$out"
```

Then I manually extracted the genes involved in HR (locus tags from the paper) or IS elements from the NSCV genomes and tblastn them against HS1

```{bash}
makeblastdb -in /lustre04/scratch/acuenod/household/01_data/16_public_seqs/NSCV1_NSCV2/HR_comparison/db/overlap_134Vc041_134Vc042.>


db=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/NSCV1_NSCV2/HR_comparison/db/overlap_134Vc041_134Vc042
query_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/NSCV1_NSCV2/HR_comparison/query/
out_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/NSCV1_NSCV2/HR_comparison/out/

for file in $(ls "$query_dir")
do
  query_name_temp="${file/%.fasta/}"
  query_name_temp="${query_name_temp/*\//}"
  query_name_temp="${query_name_temp/\:/_}"
  query_name="$query_name_temp"
  #echo "${file}"
  echo "${query_name}"
  tblastn -query "$query_dir"/"${file}" -db  "$db" \
    -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send' \
    -max_target_seqs 5000 -out "$out_dir"/"${query_name}".tab
done
```

To Val et al. and Yamamoto et al. I compared by looking at the locus tags. 

## Clair3
I use clair3 to compare all strains which have been isolated from a household where at least one fused-chromosome genome was identified. 

I first run this without specifying '--no_phasing_for_fa' which is needed for prokaryotes, so repeat for all household with this option. 

```{bash}
## household E
(run this on SP)
source ~/miniconda3/bin/activate
conda activate clair3

read_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/00_fastq
assembly_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/01_fasta
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_E.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_E

for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    # create output dir
    mkdir -p "$outdir"/"$query"/"$ref"
    # map reads with minimap2 and convert to bam file
    /mfs/acuenod/tools/minimap2/minimap2 -ax map-ont -t 16 "$assembly_dir"/"$ref".fasta "$read_dir"/"$query".fastq.gz \
    | samtools sort -o "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    samtools index "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    #samtools faidx "$assembly_dir"/"$ref".fasta
    run_clair3.sh \
    --bam_fn="$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam \
    --ref_fn="$assembly_dir"/"$ref".fasta \
    --threads=64 \
    --platform="ont" \
    --model_path="/mfs/acuenod/tools/clair3_model/r1041_e82_400bps_sup_v420" \
    --output="$outdir"/"$query"/"$ref" \
    --snp_min_af=0.8 \
    --include_all_ctgs \
    --min_mq=5 \
    --min_coverage=2 \
    --no_phasing_for_fa
  done
done

## household F
source ~/miniconda3/bin/activate
conda activate clair3

read_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/00_fastq
assembly_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/01_fasta
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_F.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_F

for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    # create output dir
    mkdir -p "$outdir"/"$query"/"$ref"
    # map reads with minimap2 and convert to bam file
    /mfs/acuenod/tools/minimap2/minimap2 -ax map-ont -t 16 "$assembly_dir"/"$ref".fasta "$read_dir"/"$query".fastq.gz \
    | samtools sort -o "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    samtools index "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    #samtools faidx "$assembly_dir"/"$ref".fasta
    run_clair3.sh \
    --bam_fn="$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam \
    --ref_fn="$assembly_dir"/"$ref".fasta \
    --threads=64 \
    --platform="ont" \
    --model_path="/mfs/acuenod/tools/clair3_model/r1041_e82_400bps_sup_v420" \
    --output="$outdir"/"$query"/"$ref" \
    --snp_min_af=0.8 \
    --include_all_ctgs \
    --min_mq=5 \
    --min_coverage=2 \
    --no_phasing_for_fa
  done
done


## household H
source ~/miniconda3/bin/activate
conda activate clair3

read_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/00_fastq
assembly_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/01_fasta
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_H.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_H

for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    # create output dir
    mkdir -p "$outdir"/"$query"/"$ref"
    # map reads with minimap2 and convert to bam file
    /mfs/acuenod/tools/minimap2/minimap2 -ax map-ont -t 16 "$assembly_dir"/"$ref".fasta "$read_dir"/"$query".fastq.gz \
    | samtools sort -o "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    samtools index "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    #samtools faidx "$assembly_dir"/"$ref".fasta
    run_clair3.sh \
    --bam_fn="$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam \
    --ref_fn="$assembly_dir"/"$ref".fasta \
    --threads=64 \
    --platform="ont" \
    --model_path="/mfs/acuenod/tools/clair3_model/r1041_e82_400bps_sup_v420" \
    --output="$outdir"/"$query"/"$ref" \
    --snp_min_af=0.8 \
    --include_all_ctgs \
    --min_mq=5 \
    --min_coverage=2 \
    --no_phasing_for_fa
  done
done

## household I
source ~/miniconda3/bin/activate
conda activate clair3

read_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/00_fastq
assembly_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/01_fasta
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_I.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_I

for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    # create output dir
    mkdir -p "$outdir"/"$query"/"$ref"
    # map reads with minimap2 and convert to bam file
    /mfs/acuenod/tools/minimap2/minimap2 -ax map-ont -t 16 "$assembly_dir"/"$ref".fasta "$read_dir"/"$query".fastq.gz \
    | samtools sort -o "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    samtools index "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    #samtools faidx "$assembly_dir"/"$ref".fasta
    run_clair3.sh \
    --bam_fn="$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam \
    --ref_fn="$assembly_dir"/"$ref".fasta \
    --threads=64 \
    --platform="ont" \
    --model_path="/mfs/acuenod/tools/clair3_model/r1041_e82_400bps_sup_v420" \
    --output="$outdir"/"$query"/"$ref" \
    --snp_min_af=0.8 \
    --include_all_ctgs \
    --min_mq=5 \
    --min_coverage=2 \
    --no_phasing_for_fa
  done
done

## household P
source ~/miniconda3/bin/activate
conda activate clair3

read_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/00_fastq
assembly_dir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/01_fasta
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_P.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_P

for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    # create output dir
    mkdir -p "$outdir"/"$query"/"$ref"
    # map reads with minimap2 and convert to bam file
    /mfs/acuenod/tools/minimap2/minimap2 -ax map-ont -t 16 "$assembly_dir"/"$ref".fasta "$read_dir"/"$query".fastq.gz \
    | samtools sort -o "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    samtools index "$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam
    #samtools faidx "$assembly_dir"/"$ref".fasta
    run_clair3.sh \
    --bam_fn="$outdir"/"$query"/"$ref"/"$query"_"$ref"_sorted.bam \
    --ref_fn="$assembly_dir"/"$ref".fasta \
    --threads=64 \
    --platform="ont" \
    --model_path="/mfs/acuenod/tools/clair3_model/r1041_e82_400bps_sup_v420" \
    --output="$outdir"/"$query"/"$ref" \
    --snp_min_af=0.8 \
    --include_all_ctgs \
    --min_mq=5 \
    --min_coverage=2 \
    --no_phasing_for_fa
  done
done


```

```{bash}
# to check if the correct option was always run, do: 
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_E
grep "\-\-no\_phasing\_for\_fa\=True" */*/run_clair3.log |wc -l # 400 --> correct
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_F
grep "\-\-no\_phasing\_for\_fa\=True" */*/run_clair3.log |wc -l # 900 --> correct
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_H
grep "\-\-no\_phasing\_for\_fa\=True" */*/run_clair3.log |wc -l # 400 --> correct
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_P
grep "\-\-no\_phasing\_for\_fa\=True" */*/run_clair3.log |wc -l # 361 --> correct
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_I
grep "\-\-no\_phasing\_for\_fa\=True" */*/run_clair3.log |wc -l # 900 --> correct

```


filter to only include hq snv

```{bash}
# summarise for household E
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_E.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_E
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_E
for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    ## create output dir
    zgrep -v "^\#" "$outdir"/"$query"/"$ref"/merge_output.vcf.gz | sed "s/$/\t"$query$ref"/" >> "$outdir"/"$query"/"$ref"/merge_output_sum.tab ; 
   done
done
# merge all and add household E
cat "$outdir"/*/*/merge_output_sum.tab | sed "s/$/\thousehold_E/" > merge_output_sum_household_E.tab


# summarise for household F
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_F.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_F
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_F
for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    ## create output dir
    zgrep -v "^\#" "$outdir"/"$query"/"$ref"/merge_output.vcf.gz | sed "s/$/\t"$query$ref"/" >> "$outdir"/"$query"/"$ref"/merge_output_sum.tab ; 
   done
done
# merge all and add household F
cat "$outdir"/*/*/merge_output_sum.tab | sed "s/$/\thousehold_F/" > merge_output_sum_household_F.tab


# summarise for household H
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_H.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_H
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_H
for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    ## create output dir
    zgrep -v "^\#" "$outdir"/"$query"/"$ref"/merge_output.vcf.gz | sed "s/$/\t"$query$ref"/" >> "$outdir"/"$query"/"$ref"/merge_output_sum.tab ; 
   done
done
# merge all and add household H
cat "$outdir"/*/*/merge_output_sum.tab | sed "s/$/\thousehold_H/" > merge_output_sum_household_H.tab



# summarise for household P
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_P.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_P
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_P
for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    ## create output dir
    zgrep -v "^\#" "$outdir"/"$query"/"$ref"/merge_output.vcf.gz | sed "s/$/\t"$query$ref"/" >> "$outdir"/"$query"/"$ref"/merge_output_sum.tab ; 
   done
done
# merge all and add household P
cat "$outdir"/*/*/merge_output_sum.tab | sed "s/$/\thousehold_P/" > merge_output_sum_household_P.tab


# summarise for household I
samples=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/samples_household_I.txt
outdir=/mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_I
cd ~/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/household_I
for query in $(cat "$samples"); do
  echo "$query"
  for ref in $(cat "$samples"); do
  echo "$ref"
    ## create output dir
    zgrep -v "^\#" "$outdir"/"$query"/"$ref"/merge_output.vcf.gz | sed "s/$/\t"$query$ref"/" >> "$outdir"/"$query"/"$ref"/merge_output_sum.tab ; 
   done
done
# merge all and add household P
cat "$outdir"/*/*/merge_output_sum.tab | sed "s/$/\thousehold_I/" > merge_output_sum_household_I.tab

# merge all
cd /mfs/acuenod/cholera/household_study/01_data/01_isolate_ONT/03_clair3/output_fusion_household/
cat household_*/merge_output_sum_household_*.tab > merge_output_sum_household_all.tab
```

eval in tree_heatmap_iqtree_UF.R


## compare chromosomes
To find out whether there is DNA transfer upon fusion / fission, I will examine the edges of the fusion sites (HS1). 
To examine if the edges are always the same, extract the flanking sides of HS1 and compare them using ANIm. 
Build bed files for 500bp up and downstream of HS1 in 'circular_chromosome_plot.R'. 

then I extracted the sequenced with: 
```{bash}
#!/bin/bash
#SBATCH --time=4:00:00
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=1G
#SBATCH --mail-user=aline.cuenod@mail.mcgill.ca
#SBATCH --mail-type=ALL
#SBATCH --account=def-shapiro-ab
#SBATCH --job-name=medaka_households

#load module 
ml StdEnv/2020 bedtools/2.29.2

samples=/lustre04/scratch/acuenod/household/01_data/samples.txt

assembly_dir=/home/acuenod/scratch/household/01_data/01_assemblies/
bed_dir=/lustre04/scratch/acuenod/household/01_data/21_check_chr_dna_transfer/flanking_HS1/beds/
output_dir=/lustre04/scratch/acuenod/household/01_data/21_check_chr_dna_transfer/flanking_HS1/seqs

# Include -s flag, forces strandedness. If the feature occupies the antisense strand, the sequence will be reverse complemented
for sample in $(cat "$samples"); do
        bedtools getfasta -fi "$assembly_dir"/"${sample}.fasta" -bed "$bed_dir"/HS1_flanking_1.bed -fo "$output_dir"/HS1_flanking_1/"$sample".fasta -s
        bedtools getfasta -fi "$assembly_dir"/"${sample}.fasta" -bed "$bed_dir"/HS1_flanking_2.bed -fo "$output_dir"/HS1_flanking_2/"$sample".fasta -s
        bedtools getfasta -fi "$assembly_dir"/"${sample}.fasta" -bed "$bed_dir"/HS1_flanking_3.bed -fo "$output_dir"/HS1_flanking_3/"$sample".fasta -s
        bedtools getfasta -fi "$assembly_dir"/"${sample}.fasta" -bed "$bed_dir"/HS1_flanking_4.bed -fo "$output_dir"/HS1_flanking_4/"$sample".fasta -s
done
```

Then I compare them using fastANI: 

```{bash}
ml StdEnv/2020 gcc/9.3.0 fastani/1.32


query_dir=/home/acuenod/scratch/household/01_data/21_check_chr_dna_transfer/flanking_HS1/seqs/
out_dir=/lustre04/scratch/acuenod/household/01_data/21_check_chr_dna_transfer/flanking_HS1/ANI/


for i in {1..4}
do
  	fastANI --ql "$query_dir"/HS1_flanking_"${i}"_paths.txt --rl "$query_dir"/HS1_flanking_"${i}"_paths.txt -o "$out_dir"/ANI_HS1_flanking_"${i}".tab -t 4 --fragLen 300
done

```

Then I check the range with: 
```{bash}
awk '{print $3}' ANI_HS1_flanking_1.tab | sort -n | awk 'NR==1 {min=$1} {max=$1} END {print "Min:", min, "Max:", max}'
```
--> above 99.9 for ANI_HS1_flanking_1, ANI_HS1_flanking_2, ANI_HS1_flanking_4. --> check ANI_HS1_flanking_3 in more detail. 
--> if ANI value below 70 its not in the output (this means there could also be differences in flanking regions 1,2 and 4) --> eval all



##pyseer
I will run a GWAS using pyseer to see if I find any specific variants to the fused chr strains

```{bash}
#install pyseer on SP
source ~/miniconda3/bin/activate
conda install pyseer
# somhow worked via pip but not conda
python3 -m pip install pyseer

# I follow this tutorial: https://pyseer.readthedocs.io/en/master/tutorial.html
# install unitig counter
conda install unitig-counter
# build unitigs
unitig-counter -strains strain_list.txt -output unitigs -nb-cores 64

# down- and uploaded the 'scripts' folder from here https://github.com/mgalardini/pyseer
# get phylogenetic distances from tree
python scripts/phylogeny_distance.py --lmm input/iqtree_all_concat_highQ_variants_no_indels_concat_snp-sites.fasta.treefile > input/phylogeny_distances.tsv
# run the actual gwas
gzip input/unitigs/unitigs.txt
pyseer --lmm --phenotypes input/pyseer_fusion.tab --kmers input/unitigs/unitigs.txt.gz --similarity input/phylogeny_distances.tsv --output-patterns output/fusion_unitig_patterns.txt --cpu 64 > output/fusion_unitigs.txt

python scripts/count_patterns.py output/fusion_unitig_patterns.txt
#gives
#Patterns:	1036
#Threshold:	4.83E-05
python scripts/qq_plot.py output/fusion_unitigs.txt

# filter for significant unitigs
cat <(head -1 output/fusion_unitigs.txt) <(awk '$4<4.83E-05 {print $0}' output/fusion_unitigs.txt) > output/significant_fusion_unitigs.txt

# to anotate I created a file listing all the gff3 file of the strains I used to compile the unitigs
# this looks like this
#/mfs/acuenod/pyseer/input/fasta/101Vc01.fasta	/mfs/acuenod/pyseer/input/gff3/101Vc01.gff3	draft
#/mfs/acuenod/pyseer/input/fasta/101Vc01.fasta	/mfs/acuenod/pyseer/input/gff3/106Vc03.gff3	draft
#/mfs/acuenod/pyseer/input/fasta/101Vc01.fasta	/mfs/acuenod/pyseer/input/gff3/111Vc05.gff3	draft
#/mfs/acuenod/pyseer/input/fasta/101Vc01.fasta	/mfs/acuenod/pyseer/input/gff3/116Vc07.gff3	draft
#/mfs/acuenod/pyseer/input/fasta/101Vc01.fasta	/mfs/acuenod/pyseer/input/gff3/121Vc09.gff3	draft
# ...

# then to annoate run
annotate_hits_pyseer output/significant_fusion_unitigs.txt input/references_list.txt output/annotated_significant_fusion_unitigs.txt

# then to summarise 
python scripts/summarise_annotations.py output/annotated_significant_fusion_unitigs.txt > output/significant_gene_hits.txt

# --> no significant gene hits found
# repeat withh all features (checked bakta output for options)
# then to annoate run
annotate_hits_pyseer --feature-type rRNA --feature-type tRNA --feature-type ncRNA --feature-type tmRNA --feature-type CDS --feature-type oriC --feature-type oriT --feature-type regulatory_region output/significant_fusion_unitigs.txt input/references_list.txt output/annotated_significant_fusion_unitigs.txt

# then to summarise 
python scripts/summarise_annotations.py output/annotated_significant_fusion_unitigs.txt > output/significant_hits_all_features.txt
# --> no other feature hits found


```



## Passaging
```{bash}
cd /home/acuenod/scratch/household/01_data/22_passaging/fastq
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode01/* > 114Vc01D00.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode02/* > 114Vc03D00.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode03/* > 117Vc01D00.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode04/* > 117Vc10D00.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode05/* > 114Vc01D03.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode06/* > 114Vc03D03.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode07/* > 117Vc01D03.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode08/* > 117Vc10D03.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode09/* > 114Vc01D08.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode10/* > 114Vc03D08.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode11/* > 117Vc01D08.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode12/* > 117Vc10D08.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode13/* > 114Vc01D11.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode14/* > 114Vc03D11.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode15/* > 117Vc01D11.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode16/* > 117Vc10D11.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode17/* > 114Vc01D16.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode18/* > 114Vc03D16.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode19/* > 117Vc01D16.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode20/* > 117Vc10D16.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode21/* > 114Vc01D20.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode22/* > 114Vc03D20.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode23/* > 117Vc01D20.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode24/* > 117Vc10D20.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode25/* > N16961.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode26/* > BS01.fastq.gz
cat /home/acuenod/scratch/household/01_data/999_raw/AW02P6/*/fastq_pass/barcode27/* > TE_Buffer.fastq.gz

# and for the assemblies
cd /home/acuenod/scratch/household/01_data/22_passaging/fasta

ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode01/assembly.fasta 114Vc01D00.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode02/assembly.fasta 114Vc03D00.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode03/assembly.fasta 117Vc01D00.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode04/assembly.fasta 117Vc10D00.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode05/assembly.fasta 114Vc01D03.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode06/assembly.fasta 114Vc03D03.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode07/assembly.fasta 117Vc01D03.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode08/assembly.fasta 117Vc10D03.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode09/assembly.fasta 114Vc01D08.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode10/assembly.fasta 114Vc03D08.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode11/assembly.fasta 117Vc01D08.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode12/assembly.fasta 117Vc10D08.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode13/assembly.fasta 114Vc01D11.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode14/assembly.fasta 114Vc03D11.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode15/assembly.fasta 117Vc01D11.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode16/assembly.fasta 117Vc10D11.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode17/assembly.fasta 114Vc01D16.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode18/assembly.fasta 114Vc03D16.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode19/assembly.fasta 117Vc01D16.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode20/assembly.fasta 117Vc10D16.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode21/assembly.fasta 114Vc01D20.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode22/assembly.fasta 114Vc03D20.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode23/assembly.fasta 117Vc01D20.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode24/assembly.fasta 117Vc10D20.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode25/assembly.fasta N16961.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode26/assembly.fasta BS01.fasta
ln -s /lustre04/scratch/acuenod/household/01_data/999_raw/AW02P6/assemblies/barcode27/assembly.fasta TE_Buffer.fasta

```

I then map them against HS1 in its fused and unfused form: 

```{bash}
#SBATCH --array=1-27 # indicate the number of samples to be analysed here. 
array=(mock 114Vc01D00 114Vc03D00 117Vc01D00 117Vc10D00 114Vc01D03 114Vc03D03 117Vc01D03 117Vc10D03 114Vc01D08 114Vc03D08 117Vc01D08 117Vc10D08 114Vc01D11 114Vc03D11 117Vc01D11 117Vc10D11 114Vc01D16 114Vc03D16 117Vc01D16 117Vc10D16 114Vc01D20 114Vc03D20 117Vc01D20 117Vc10D20 N16961 BS01 TE_Buffer)

# load modules
ml StdEnv/2020 minimap2/2.24

# define the variable 'sample_id'from the 'array' defined above
export sample=${array["$SLURM_ARRAY_TASK_ID"]}

input_reads=/home/acuenod/scratch/household/01_data/22_passaging/fastq/"$sample".fastq.gz
input_ref=/home/acuenod/scratch/household/01_data/01_assemblies/135Vc05.fasta # map all against 135Vc05 (is from the same patient as 135Vc06, which was used for the read extraction for the fused chromosome, but assembled into two chromosomes) 
output_dir=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_non-fused_HR

# run minimap
minimap2 -ax map-ont --sam-hit-only -t 16 "$input_ref" "$input_reads" > "$output_dir"/"$sample".sam # for Oxford Nanopore reads

# run sam to bam and index
ml StdEnv/2020 samtools/1.17
# convert sam to bam
samtools view -S -b "$output_dir"/"$sample".sam > "$output_dir"/"$sample".bam
# sort bam file
samtools sort "$output_dir"/"$sample".bam > "$output_dir"/"$sample".sorted.bam
# index sorted bam file
samtools index "$output_dir"/"$sample".sorted.bam

#repeat for fusion site
#
input_reads=/home/acuenod/scratch/household/01_data/22_passaging/fastq/"$sample".fastq.gz
input_ref2=/home/acuenod/scratch/household/01_data/01_assemblies/135Vc06.fasta # map all against 135Vc06 
output_dir2=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_fused_HR

# run minimap
minimap2 -ax map-ont --sam-hit-only -t 16 "$input_ref2" "$input_reads" > "$output_dir2"/"$sample".sam # for Oxford Nanopore reads

# run sam to bam and index
ml StdEnv/2020 samtools/1.17
# convert sam to bam
samtools view -S -b "$output_dir2"/"$sample".sam > "$output_dir2"/"$sample".bam
# sort bam file
samtools sort "$output_dir2"/"$sample".bam > "$output_dir2"/"$sample".sorted.bam
# index sorted bam file
samtools index "$output_dir2"/"$sample".sorted.bam
```

```{bash}
# clean up
rm *.sam
rm *0.bam
rm *1.bam
rm *3.bam
rm *8.bam
rm *16.bam
```

Then filter the reads as before:
For the non-fused:
```{bash}
array=(mock 114Vc01D00 114Vc03D00 117Vc01D00 117Vc10D00 114Vc01D03 114Vc03D03 117Vc01D03 117Vc10D03 114Vc01D08 114Vc03D08 117Vc01D08 117Vc10D08 114Vc01D11 114Vc03D11 117Vc01D11 117Vc10D11 114Vc01D16 114Vc03D16 117Vc01D16 117Vc10D16 114Vc01D20 114Vc03D20 117Vc01D20 117Vc10D20 N16961 BS01 TE_Buffer)

export sample=${array["$SLURM_ARRAY_TASK_ID"]}

#load apptainer module 
ml  StdEnv/2020 samtools/1.17

input=/home/acuenod/scratch/household/01_data/22_passaging/readmapping_non-fused_HR/"$sample".sorted.bam
input_bed_HR_chr=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_HR_non-fused/135Vc05_junctions_extended_HR_13087.bed #all were mapped agains 135Vc06 (HR_chr position always stays the same)
output_bed=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_non-fused_HR/beds_out/"$sample".bed
output_bed_no_inserts=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_non-fused_HR/beds_out/"$sample"_no_inserts_larger_1000.bed
output_readID_HR_chr_1=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_non-fused_HR/read_ids/"$sample"_reads_spanning_HR1.txt
output_readID_HR_chr_2=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_non-fused_HR/read_ids/"$sample"_reads_spanning_HR2.txt


# bam to bed
ml bedtools/2.29.2
# convert bam to bed and filter bed for reads. remove duplicated reads and non-primary alignments
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |bedtools bamtobed > "$output_bed"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}\D)/' |bedtools bamtobed > "$output_bed_no_inserts"
# this second line filters reads out which have an deletion (when compared to the reference of 1000 or more. (this first awk expression extracts the headers, the second excludes reads which have a 4-digit-D pattern in their CIGAR string)

#bedtools bamtobed -i "$input" > "$output_bed"

# extract reads spanning HR_chr 1
HR_chr_1_1=$(awk '$4== "HR_chr_1" {print $2}' "$input_bed_HR_chr")
HR_chr_1_2=$(awk '$4== "HR_chr_1" {print $3}' "$input_bed_HR_chr")
chr1_name=$(awk '$4== "HR_chr_1" {print $1}' "$input_bed_HR_chr")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' |bedtools bamtobed | awk -v var0="$chr1_name" -v var1="$HR_chr_1_1" -v var2="$HR_chr_1_2" '{ if ($1 == var0 && $2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"HR_chr_1":"HR_chr_1")} 1' > "$output_readID_HR_chr_1"

# extract reads spanning HR_chr 1
HR_chr_2_1=$(awk '$4== "HR_chr_2" {print $2}' "$input_bed_HR_chr")
HR_chr_2_2=$(awk '$4== "HR_chr_2" {print $3}' "$input_bed_HR_chr")
chr2_name=$(awk '$4== "HR_chr_2" {print $1}' "$input_bed_HR_chr")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' |bedtools bamtobed | awk -v var0="$chr2_name" -v var1="$HR_chr_2_1" -v var2="$HR_chr_2_2" '{ if ($1 == var0 && $2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"HR_chr_2":"HR_chr_2")} 1' > "$output_readID_HR_chr_2"

```

For the fused: 
```{bash}
array=(mock 114Vc01D00 114Vc03D00 117Vc01D00 117Vc10D00 114Vc01D03 114Vc03D03 117Vc01D03 117Vc10D03 114Vc01D08 114Vc03D08 117Vc01D08 117Vc10D08 114Vc01D11 114Vc03D11 117Vc01D11 117Vc10D11 114Vc01D16 114Vc03D16 117Vc01D16 117Vc10D16 114Vc01D20 114Vc03D20 117Vc01D20 117Vc10D20 N16961 BS01 TE_Buffer)


export sample=${array["$SLURM_ARRAY_TASK_ID"]}

#load apptainer module 
ml  StdEnv/2020 samtools/1.17

input=/home/acuenod/scratch/household/01_data/22_passaging/readmapping_fused_HR/"$sample".sorted.bam
input_bed_junction=/lustre04/scratch/acuenod/household/01_data/13_minimap_alignmnets/read_id_spanning_junction_HR/135Vc06_junctions_extended_HR_13087.bed #all were mapped agains 135Vc06 (junction position always stays the same)
output_bed=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_fused_HR/beds_out/"$sample".bed
output_bed_no_inserts=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_fused_HR/beds_out/"$sample"_no_inserts_larger_1000.bed
output_readID_junction_1=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_fused_HR/read_ids/"$sample"_reads_spanning_HR1.txt
output_readID_junction_2=/lustre04/scratch/acuenod/household/01_data/22_passaging/readmapping_fused_HR/read_ids/"$sample"_reads_spanning_HR2.txt


# bam to bed
ml bedtools/2.29.2
# convert bam to bed and filter bed for reads. remove duplicated reads and non-primary alignments
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |bedtools bamtobed > "$output_bed"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP |awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}\D)/' |bedtools bamtobed > "$output_bed_no_inserts"

#bedtools bamtobed -i "$input" > "$output_bed"
# extract reads spanning junction 1
junction_1_1=$(awk '$4== "junction_1" {print $2}' "$input_bed_junction")
junction_1_2=$(awk '$4== "junction_1" {print $3}' "$input_bed_junction")
#bedtools bamtobed -i "$input" | awk -v var1="$junction_1_1" -v var2="$junction_1_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_1":"junction_1")} 1' > "$output_readID_junction_1"
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP | awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' | bedtools bamtobed | awk -v var1="$junction_1_1" -v var2="$junction_1_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_1":"junction_1")} 1' > "$output_readID_junction_1"

# extract reads spanning junction 1
junction_2_1=$(awk '$4== "junction_2" {print $2}' "$input_bed_junction")
junction_2_2=$(awk '$4== "junction_2" {print $3}' "$input_bed_junction")
samtools view "$input" --min-MQ 1 -F 256 -h --remove-flags DUP | awk 'substr($0,1,1)=="@" || $6 !~ /([0-9]{4,}D)/{print}' | bedtools bamtobed | awk -v var1="$junction_2_1" -v var2="$junction_2_2" '{ if ($2 <= var1  && $3 >= var2) print $4 }'  | awk '!seen[$0]++' | awk '{$1 = $1 OFS (NR==1?"junction_2":"junction_2")} 1' > "$output_readID_junction_2"

```

Then count the number of reads for both mapping scenarios:
```{bash}
cd /home/acuenod/scratch/household/01_data/22_passaging
wc -l readmapping_*_HR/read_ids/* > count_reads_HS1_passaging.txt
```

summarise the flye output to get information on mean read depth to normalize read count

```{bash}
cd /home/acuenod/scratch/household/01_data/999_raw/AW02P6/assemblies

for sample in $(cat barcodes.txt)
do
  echo ${sample}
  tail -n+2 ${sample}*/*assembly_info.txt | awk -F "\t" -v samplezz="$sample" '{OFS="\t"} {print $0, samplezz, refzz}' >> ../../../22_passaging/fasta/assembly_info_all.txt
done 
```

--> not all yielded an assembly (two failed because of not enoug reads, I repeated and checked). Therefore, use total read length to normalize

```{bash}
grep "Total read length" barcode*/flye.log > total_read_length_info.txt
# remove duplicate lines
awk -i inplace '!seen[$0]++' total_read_length_info.txt
```




# Analysis of publicly available sequence data
## Short reads
### Assembly and QC
Reads are assembled using Spades via Unicycler and Quality controlled via Quast, screened for contamination using Metaphlan and annotated via bakta:

```{bash, echo=T, eval=F}

# This script has been adapted from a script originally written by Dr. Daniel WÃ¼thrich in the research group of Prof. Adrian Egli (aegli@imm.uzh.ch) 

# set directory where the metaphlan database is copied to (ther database has to live in scratch and can be copied from '/lustre03/project/6077363/GROUP/01_db/metaphlan_databases.tar.gz' and needs to be tar unzipped before usage)
metaphlan_db_dir=/scratch/acuenod/metaphlan_databases/
# set directory where the virtual environmets 'unicycler' and 'metaphlan' are copied to. Both can be copied from '/lustre03/project/6077363/GROUP/02_software/01_tools/venvs/' 
venv_dir=$HOME/software/venvs
bakta_db=/lustre03/project/6077363/GROUP/01_db/bakta/

# unload all
ml purge

# define the variable 'sample_id'from the 'array' defined above
export sample_id=${array["$SLURM_ARRAY_TASK_ID"]}

# assign raw reads
raw_R1=../reads/"$sample_id"_1.fastq.gz
raw_R2=../reads/"$sample_id"_2.fastq.gz

# trim reads using trimmomatic
mkdir -p results/"$sample_id"/0_trimming
export _JAVA_OPTIONS="-Xmx10g"
module load trimmomatic/0.39 # load module
java -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar PE -threads "$SLURM_CPUS_PER_TASK" -phred33 "$raw_R1" "$raw_R2" results/"$sample_id"/0_trimming/r1.fastq.gz results/"$sample_id"/0_trimming/r1.not-paired.fastq.gz results/"$sample_id"/0_trimming/r2.fastq.gz results/"$sample_id"/0_trimming/r2.not-paired.fastq.gz ILLUMINACLIP:TruSeq3-PE:2:30:10 SLIDINGWINDOW:4:12 MINLEN:100 2> results/"$sample_id"/0_trimming/quality_read_trimm_info
module unload trimmomatic/0.39

# assigne trimmed reads to new variable
R1=results/"$sample_id"/0_trimming/r1.fastq.gz
R2=results/"$sample_id"/0_trimming/r2.fastq.gz

# assemble using spades via unicycler
mkdir -p results/"$sample_id"/1_unicycler
# load modules
source "$venv_dir"/unicycler/bin/activate
module load StdEnv/2020  gcc/9.3.0
module load blast+/2.13.0
module load spades/3.15.4
unicycler -t "$SLURM_CPUS_PER_TASK" -1 "$R1" -2 "$R2" -o results/"$sample_id"/1_unicycler # optional long reads #  -l ../reads/"$sample_id".fastq.gz
deactivate
module unload blast+/2.13.0
module unload spades/3.15.4

# annotate using bakta
module load StdEnv/2020  gcc/9.3.0 trnascan-se/2.0.12
module load aragorn/1.2.41
module load infernal/1.1.4
module load hmmer/3.3.2
module load diamond/2.0.15
module load blast+/2.13.0
module load circos/0.69-9
# add piler and amrfinder+ commands to path
export PATH=/home/acuenod/software/venvs/amr:/home/acuenod/software/venvs/pilercr1.06:$PATH
source "$venv_dir"/bakta/bin/activate
bakta  --verbose --db "$bakta_db" results/"$sample_id"/1_unicycler/assembly.fasta --output results/"$sample_id"/2_annotation --force
deactivate

mkdir -p results/"$sample_id"/3_quality

#get assembly stats using quast
module load quast/5.0.2
quast --min-contig 0 -o results/"$sample_id"/3_quality/quast/ results/"$sample_id"/2_annotation/"$sample_id".fna
module load quast/5.0.2

# Assign species and check for containations with Metaphlan
# load modules
module load gcc blast samtools bedtools bowtie2 python/3.10
source "$venv_dir"/metaphlan/bin/activate
mkdir -p results/"$sample_id"/3_quality/Metaphlan2
#count the number of reads
n_reads_R1=$(echo $(cat "$R1"|wc -l)/4|bc) # for forward reads
n_reads_R2=$(echo $(cat "$R2"|wc -l)/4|bc) # for reverse reads
n_reads=$(echo "$(($n_reads_R1+$n_reads_R2))") # add both
# map reads to db
bowtie2 --sam-no-hd --sam-no-sq --no-unal --very-sensitive -S results/"$sample_id"/3_quality/Metaphlan2/alignment.sam -x "$metaphlan_db_dir"/mpa_vOct22_CHOCOPhlAnSGB_202212 -1 "$R1"  -2 "$R2"
#run meaphlan from sam file
metaphlan results/"$sample_id"/3_quality/Metaphlan2/alignment.sam --input_type sam --index mpa_vOct22_CHOCOPhlAnSGB_202212  --bowtie2db "$metaphlan_db_dir" --nproc "$SLURM_CPUS_PER_TASK" --nreads "$n_reads" --offline> results/"$sample_id"/3_quality/Metaphlan2/profiled_metagenome.txt 2> results/"$sample_id"/3_quality/Metaphlan2/error.txt
deactivate
module unload gcc blast samtools bedtools bowtie2 python/3.10

#remap the trimmed reads to the assembly (using bwa) to calculate read depth
R1=results/"$sample_id"/0_trimming/r1.fastq.gz
R2=results/"$sample_id"/0_trimming/r2.fastq.gz

# load modules
module load bwa/0.7.17
module load samtools/1.17
mkdir -p results/"$sample_id"/3_quality/remapping/mapping/
# create a symlink of the assembly to the mapping dir, so that the sam file will be in the correct dir (somehow could not redirect?)
assemblypath=$(realpath results/"$sample_id"/2_annotation/"$sample_id".fna)
ln -s "$assemblypath" results/"$sample_id"/3_quality/remapping/mapping/"$sample_id".fna
# index assembly
bwa index results/"$sample_id"/3_quality/remapping/mapping/"$sample_id".fna results/"$sample_id"/3_quality/remapping/mapping/"$sample_id".fna
# map
bwa mem -t "$SLURM_CPUS_PER_TASK" results/"$sample_id"/3_quality/remapping/mapping/"$sample_id".fna "$R1" "$R2" > results/"$sample_id"/3_quality/remapping/mapping/alingnment.sam
# sort the sam file and convert to bam
samtools sort -@ "$SLURM_CPUS_PER_TASK" -T results/"$sample_id"/3_quality/remapping/mapping/temp_sort -o results/"$sample_id"/3_quality/remapping/mapping/alingnment.bam results/"$sample_id"/3_quality/remapping/mapping/alingnment.sam
# remove duplicates (not sure?) and index
samtools rmdup results/"$sample_id"/3_quality/remapping/mapping/alingnment.bam results/"$sample_id"/3_quality/remapping/mapping/alingnment.removed_duplicates.bam
samtools index results/"$sample_id"/3_quality/remapping/mapping/alingnment.removed_duplicates.bam
# calculate read depth
samtools depth -aa results/"$sample_id"/3_quality/remapping/mapping/alingnment.removed_duplicates.bam | awk '{sum+=$3;count+=1;}END{print sum/count;}'  > results/"$sample_id"/3_quality/remapping/coverage.tab
module unload bwa/0.7.17
module unload samtools/1.17


# remove files no longer needed
rm results/"$sample_id"/0_trimming/*.fastq.gz
rm results/"$sample_id"/3_quality/Metaphlan2/alignment.sam
rm results/"$sample_id"/3_quality/remapping/mapping/*

# summarise quality stats
mkdir results/"$sample_id"/3_quality/summary
# fetch reads quality from trimmomatics out
awk '/Input Read Pairs/{print $8}' results/"$sample_id"/0_trimming/quality_read_trimm_info | awk -F '%'  '{print $1}' | awk -F '('  '{print $2}' > results/"$sample_id"/3_quality/summary/1.txt
# fetch the read depth from the remapping
cp results/"$sample_id"/3_quality/remapping/coverage.tab results/"$sample_id"/3_quality/summary/2.txt
# fetch coontig count from quast
tail -n 1 results/"$sample_id"/3_quality/quast/transposed_report.tsv | awk '{print $14 "\t" $16  "\t" $18  "\t" $17 }' > results/"$sample_id"/3_quality/summary/3.txt
# fetch metaphlan profiles
grep "s__" results/"$sample_id"/3_quality/Metaphlan2/profiled_metagenome.txt  | grep -v "t__" | awk -F '[|\t]+' '{print $7,"\011"$15}' | awk -F __ '{print $2}'| head -n 1 > results/"$sample_id"/3_quality/summary/4.txt

# summaris quality stats for all samples
paste <(echo "$sample_id") results/"$sample_id"/3_quality/summary/*.txt > results/"$sample_id"/3_quality/summary/"$sample_id".tab
echo -e "Sample\tRead_quality\tRead_depth\tContig_count\tTotal_length\tN50\tGC_percent\tAlignment_length\tMetaPhlAn2_species\tMetaPhlAn2_purity" > quality.tab
cat results/*/3_quality/summary/*.tab >> quality.tab
# count the number of samples analysed
let sample_count=$(grep -c "" quality.tab)-1
echo "analysed_samples: $sample_count" >> quality.tab
# convert tab to csv for export
sed 's/,/;/' quality.tab | sed 's/\t/,/g' > quality.csv

```


### Phylogeny
I used panaroo to compile the core genome alignment from which a tree was created using fastree. 
panaroo: 

```{bash, echo=T, eval=F}
input_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/gff3
output_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/24-02_panaroo_out/

panaroo -i "$input_dir"/*.gff3 -o "$output_dir" --clean-mode moderate --remove-invalid-genes --threads 16 --alignment core --aligner mafft

```

snp-sites and fasttree: 
```{bash, echo=T, eval=F}
ml StdEnv/2020 snp-sites/2.5.1
snp-sites -c  /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/panaroo_out/core_gene_alignment_filtered.aln > /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/panaroo_out/core_gene_alignment_filtered-snp-sites.aln

# load module
ml StdEnv/2020 fasttree/2.1.11


FastTree -gtr -nt /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/panaroo_out/core_gene_alignment_filtered-snp-sites.aln > /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/panaroo_out/fasttree


```

### Lineage assignment
Is 7PET to determine whether a genome belongs to the 7PET sublineage: 

```{bash, echo=T, eval=F}
# load ariva venv
source /home/acuenod/software/venvs/ariba/bin/activate
# load modules (dependencies for ariba)
ml StdEnv/2020 python/3.10.2 bowtie2/2.4.1 cd-hit/4.8.1 mummer/4.0.0beta2
# for snippy
ml java/13.0.2
# other
ml gcc/9.3.0 blast+/2.13.0
ml r/4.0.0
ml mash/2.3

/home/acuenod/software/venvs/IsIt7PET/find7PET_ToolPaths_adapted_AC.sh /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/IsIt7PET/input/read_paths.tab

```

To identify to which 7PET sublineage the genomes are closest to, I used snippy. I assigned the lineage of the reference for which the lowest number of SNV were identified: 

```{bash, echo=T, eval=F, echo=T, eval=F}
threads=16

# define the variable 'sample_id'from the 'array' defined above
export sample_id=${array["$SLURM_ARRAY_TASK_ID"]}

## define read directory
read_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/fastqref_dir=/lustre04/scratch/acuenod/household/01_data/17_lineage_assignment/refs/fasta/
outdir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/medaka_lineage/out/

# load modules
ml java/17.0.2

for ref in $(ls "$ref_dir")
do
# use minqual 100, as its all illumina data
/home/acuenod/software/scripts/snippy/bin/snippy --cpus 16 --outdir "$outdir"/"$sample_id"/"$ref" --ref "$ref_dir"/"$ref" --R1 "$read_dir"/"$sample_id"_1.fastq.gz --R2 "$read_dir"/"$sample_id"_2.fastq.gz --mincov 10 --minfrac 0.7 --minqual 100 --force
done

# use the following to summarise the number of SNV per reference: 
cd /home/acuenod/scratch/household/01_data/16_public_seqs/highQ_short_read_datasets/medaka_lineage/out
for sample in $(ls .)
do
  echo ${sample}
  for ref in $(ls ${sample})
  do
    echo ${ref}
    #ls 2>&1 ${sample}/${ref}/snps.txt >> log.txt
    tail -n 4 ${sample}/${ref}/snps.txt | awk -F "\t" -v samplezz="$sample" -v refzz="$ref" '{OFS="\t"} {print $0, samplezz, refzz}' > ${sample}/${ref}/snps_1.txt
  done
done

cat */*/snps_1.txt > all_snps_1.txt
sed -i '1i \ variant\tsample\treference' all_snps_1.txt

```




### Screening homologous sequences (HS1)
I used the following to screen HS1 and VPI-I in all public shoort read sequences

```{bash, echo=T, eval=F}
db=/home/acuenod/scratch/household/01_data/16_public_seqs/highQ_short_read_datasets/blast/input/db/public_Vc_db
query=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/blast/input/query/Vc_over_10000_chr_overlap_1.fasta
out=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/blast/output/blastout_HR_2_all_shortread_public.tab

blastn -query "$query" -db  "$db" \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send' \
  -max_target_seqs 5000 -out "$out"

```

from these blastoutputs, I created bed files using 'blast_out_to_bed_HR_public.R'.
I then calculated the read depths in these (using the bamfiles previously created) with:

```{bash, echo=T, eval=F}
#load apptainer module 
module load bwa/0.7.17
module load samtools/1.17

assemblypath=/home/acuenod/scratch/household/01_data/16_public_seqs/highQ_short_read_datasets/fasta_all
readpath=/home/acuenod/scratch/household/01_data/16_public_seqs/highQ_short_read_datasets/fastq_all
calc_read_depth_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/readmapping/calc_read_depth
mapping_reads_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/readmapping/calc_read_depth
input_bed=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/readmapping/calc_read_depth/location_HR2.bed


## index assembly
#bwa index "$assemblypath"/"$sample".fna "$assemblypath"/"$sample".fna
## map
#bwa mem -t "$SLURM_CPUS_PER_TASK" "$assemblypath"/"$sample".fna "$readpath"/"$sample"_1.fastq.gz "$readpath"/"$sample"_2.fastq.gz > "$mapping_reads_dir"/"$sample".sam
## sort the sam file and convert to bam
#samtools sort -@ "$SLURM_CPUS_PER_TASK" -T "$mapping_reads_dir"/tempsort -o  "$mapping_reads_dir"/"$sample".bam  "$mapping_reads_dir"/"$sample".sam
## remove duplicates (not sure?) and index
#samtools rmdup "$mapping_reads_dir"/"$sample".bam "$mapping_reads_dir"/"$sample".removed_duplicates.bam
#samtools index "$mapping_reads_dir"/"$sample".removed_duplicates.bam
#samtools sort -@ "$SLURM_CPUS_PER_TASK" "$mapping_reads_dir"/"$sample".removed_duplicates.bam > "$mapping_reads_dir"/"$sample".removed_duplicates.sorted.bam


## calculate read depth
# samtools depth -aa "$mapping_reads_dir"/"$sample".removed_duplicates.sorted.bam   > "$calc_read_depth_dir"/"$sample"_all.coverage
for sample in $(cat /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/HR2_samples.txt)
do
    ## calculate coverage of contig_2 seq region 
    samtools depth -b "$input_bed"  "$mapping_reads_dir"/"$sample".removed_duplicates.bam > "$calc_read_depth_dir"/"$sample"_HR2_region.coverage
    # summarise average coverage
    full_cov=$(awk -F'\t' '{ sum += $3 } END { print sum / NR }' "$calc_read_depth_dir"/"$sample"_all.coverage)
    subset_cov=$(awk -F'\t' '{ sum += $3 } END { print sum / NR }' "$calc_read_depth_dir"/"$sample"_HR_region.coverage)
    subset2_cov=$(awk -F'\t' '{ sum += $3 } END { print sum / NR }' "$calc_read_depth_dir"/"$sample"_HR2_region.coverage)
    echo -e   "Average_coverage_full_genome\tAverage_coverage_of_HR_region\tAverage_coverage_of_HR2_region"  > "$calc_read_depth_dir"/"$sample"_average_coverage_both.tab
    printf "%s\t%s\t" "$full_cov" "$subset_cov"  "$subset2_cov" >> "$calc_read_depth_dir"/"$sample"_average_coverage_both.tab
done

```



and summarised these using the following: 
```{bash, echo=T, eval=F}
cd /lustre04/scratch/acuenod/household/01_data/16_public_seqs/highQ_short_read_datasets/readmapping/calc_read_depth/
# add filename to al as column
for file in $(ls *_average_coverage_both.tab)
do 
  samplename="${file/%_average_coverage_both.tab/}"
  echo "$samplename"
  awk 'NR == 1 {print $0 "\tname_file"; next;}{print $0 "\t" FILENAME;}' "$file" > "$samplename"_average_coverage_both2.tab
done
# concatenate all
cat *_average_coverage_both2.tab > all_average_coverage_both.tab
# remove all but first header line
sed -i -e '/^Average\_coverage/{x;/./!{x;h;b;};d}' all_average_coverage_both.tab
rm *_average_coverage_both2.tab

```


## Long reads
### QC
I QCed the assembled long read Vibrionales spp. using the following: 

```{bash, echo=T, eval=F}
# This script has been adapted from a script originally written by Dr. Daniel WÃ¼thrich in the research group of Prof. Adrian Egli (aegli@imm.uzh.ch) 

# set directory where environments are installed to, eg nanostats
venv_dir=/home/acuenod/software/venvs/

# unload all
ml purge

# define the variable 'sample_id'from the 'array' defined above
export sample_id=${array["$SLURM_ARRAY_TASK_ID"]}

# go to folder
cd /home/acuenod/scratch/household/01_data/16_public_seqs/long_reads
#cd /home/acuenod/scratch/ONT

# assign reads to new variable
R=00_reads/"$sample_id".fastq.gz

# assign assembly to variable
assembly=assemblies/fasta/"$sample_id".fasta
#assembly=01_assemblies/"$sample_id"/"$sample_id".fasta

mkdir -p 02_quality/"$sample_id"

# get read stats using nanostat
source "$venv_dir"/nanostats/bin/activate
NanoStat -t "$SLURM_CPUS_PER_TASK" --fastq "$R" --outdir 02_quality/"$sample_id"/nanostat/ --name "$sample_id" --tsv
deactivate

## get read stats using seqtk
#ml nixpkgs/16.09  gcc/7.3.0 seqtk/1.3
#mkdir 02_quality/"$sample_id"/seqtk_fqchk
#seqtk fqchk "$R" > 02_quality/"$sample_id"/seqtk_fqchk/"$sample_id".tab
#module unload nixpkgs/16.09  gcc/7.3.0 seqtk/1.3


##get assembly stats using quast
module load StdEnv/2020  gcc/9.3.0 quast/5.0.2
quast --min-contig 0 -o 02_quality/"$sample_id"/quast/ "$assembly"
module unload quast/5.0.2

#remap the reads to the assembly (using minimap) to calculate read depth
# load modules
module load bwa/0.7.17
module load samtools/1.17
module load minimap2/2.24
mkdir -p 02_quality/"$sample_id"/remapping/mapping/
cp "$assembly" 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna
# index assembly
bwa index 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna
# map
minimap2 -ax map-ont -t "$SLURM_CPUS_PER_TASK" 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna "$R"  > 02_quality/"$sample_id"/remapping/mapping/alingnment.sam
# sort the sam file and convert to bam
samtools sort -@ "$SLURM_CPUS_PER_TASK" -T 02_quality/"$sample_id"/remapping/mapping/temp_sort -o 02_quality/"$sample_id"/remapping/mapping/alingnment.bam 02_quality/"$sample_id"/remapping/mapping/alingnment.sam
# remove duplicates (not sure?) and index
samtools rmdup 02_quality/"$sample_id"/remapping/mapping/alingnment.bam 02_quality/"$sample_id"/remapping/mapping/alingnment.removed_duplicates.bam
samtools index 02_quality/"$sample_id"/remapping/mapping/alingnment.removed_duplicates.bam
# calculate read depth
samtools depth -aa 02_quality/"$sample_id"/remapping/mapping/alingnment.removed_duplicates.bam | awk '{sum+=$3;count+=1;}END{print sum/count;}'  > 02_quality/"$sample_id"/remapping/coverage.tab
module unload bwa/0.7.17
module unload samtools/1.17
rm 02_quality/"$sample_id"/remapping/mapping/"$sample_id".fna

### remove files no longer needed
##rm results/"$sample_id"/3_quality/Metaphlan2/alignment.sam
##rm results/"$sample_id"/3_quality/remapping/mapping/*

## summarise quality stats
mkdir 02_quality/"$sample_id"/summary
## fetch median read length, n50 and median read quality from nanostats
awk '$1 == "median_read_length" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1a.txt
awk '$1 == "n50" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1b.txt
awk '$1 == "median_qual" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1c.txt
awk '$1 == "mean_qual" {print $2}' 02_quality/"$sample_id"/nanostat/"$sample_id" > 02_quality/"$sample_id"/summary/1d.txt
awk '$1 == "ALL" {print $8}' 02_quality/"$sample_id"/seqtk_fqchk/"$sample_id".tab > 02_quality/"$sample_id"/summary/1e.txt
# fetch the read depth from the remapping
cp 02_quality/"$sample_id"/remapping/coverage.tab 02_quality/"$sample_id"/summary/2.txt
# fetch coontig count from quast
tail -n 1 02_quality/"$sample_id"/quast/transposed_report.tsv | awk '{print $14 "\t" $16  "\t" $18  "\t" $17 }' > 02_quality/"$sample_id"/summary/3.txt
# summaris quality stats for all samples
paste <(echo "$sample_id") 02_quality/"$sample_id"/summary/*.txt > 02_quality/"$sample_id"/summary/"$sample_id".tab
echo -e "Sample\tMedian_read_length\tN50_reads\tMedian_read_quality_nanostat\tMean_read_quality_nanostat\tMean_read_quality_seqtk\tRead_depth\tContig_count\tTotal_length_assembly\tN50_assembly\tGC_percent" >  02_quality/quality.tab
cat 02_quality/*/summary/*.tab >> 02_quality/quality.tab
# count the number of samples analysed
let sample_count=$(grep -c "" 02_quality/quality.tab)-1
echo "analysed_samples: $sample_count" >> 02_quality/quality.tab
# convert tab to csv for export
sed 's/,/;/' 02_quality/quality.tab | sed 's/\t/,/g' > 02_quality/quality.csv
```

### species classification
I used GTDB-tk for species assignment

```{bash, echo=T, eval=F}


# load modules
ml StdEnv/2020 prodigal/2.6.3
ml gcc/9.3.0 hmmer/3.3.2
ml pplacer/1.1.alpha19
ml fastani/1.32
ml fasttree/2.1.11
ml mash/2.3

source /home/acuenod/software/venvs/GTDB-Tk/bin/activate

# give path to ref data 
#export GTDBTK_DATA_PATH=/lustre04/scratch/acuenod/release214
export GTDBTK_DATA_PATH=/lustre03/project/6033517/acuenod/household_current/gtdbtk_db/release214
gtdbtk classify_wf --cpus 8 --skip_ani_screen --extension fasta --genome_dir /home/acuenod/scratch/household/01_data/16_public_seqs/long_reads/assemblies/fasta_added/ --out_dir /lustre03/project/6033517/acuenod/household_current/GTDB-Tk_out_2
```
 
And kraken/bracken to screen for contamintations (I used this on the assemblies and not on the reads, which is probably not ideal)

```{bash, echo=T, eval=F}
# kraken
db=/lustre03/project/6033517/acuenod/household_current/kraken_db/lustre04/scratch/acuenod/kraken2_db
assembly_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/assemblies/fasta_added
out_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/kraken_out_added

for sample in $(ls "$assembly_dir"/*fasta|sed 's/.*fasta\_added\///g'| sed 's/\.fasta//g')
do
  echo "$sample"
  kraken2 --db "$db" "$assembly_dir"/"$sample".fasta  --threads 8 --report "$out_dir"/"$sample"_kraken2_report.txt > "$out_dir"/"$sample"_kraken2_report.txt
done

# bracken
# load modules
ml StdEnv/2020  gcc/9.3.0 bracken/2.6.0

export sample_id=${array["$SLURM_ARRAY_TASK_ID"]}

bracken_out2=/lustre03/project/6033517/acuenod/household_current/bracken_out
kraken_out2=/lustre03/project/6033517/acuenod/household_current/kraken_out

bracken -d /home/acuenod/scratch/kraken2_db -i "$kraken_out2"/"$sample_id"_kraken2_report.txt -o "$bracken_out2"/"$sample_id".bracken -r 150 -l S


```

 
### compare chr 1 and chr 2
#### blast
I identified assemblies with two large contigs (corresoponding to two chromosomes) in 'public_long_reads.R'
I will next blast chr.2 agains chr.1 for each sample. 
Therefore, I extracted all chr.1 and chr.2 names in 'public_long_reads.R' and extracted these from the assemblies using the following (in an interactive job)

```{bash, echo=T, eval=F}
ml nixpkgs/16.09  gcc/7.3.0 seqtk/1.3

assembly_dir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/assemblies/fasta
chr1_names=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/blast_chr/chr1/chr1_names.txt
chr1_out_dir=/home/acuenod/scratch/household/01_data/16_public_seqs/long_reads/blast_chr/chr1
chr2_names=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/blast_chr/chr2/chr2_names.txt
chr2_out_dir=/home/acuenod/scratch/household/01_data/16_public_seqs/long_reads/blast_chr/chr2


for sample in  $(cat /lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/blast_chr/assemblies_2_2chr.txt)
do
  echo "$sample"
  seqtk subseq "$assembly_dir"/"$sample".fasta "$chr1_names" > "$chr1_out_dir"/"$sample"_chr_1.fasta
  seqtk subseq "$assembly_dir"/"$sample".fasta "$chr2_names" > "$chr2_out_dir"/"$sample"_chr_2.fasta
done
```

For the actual blast: 

```{bash, echo=T, eval=F}
ml StdEnv/2020 gcc/9.3.0 blast+/2.13.0

chr1_dir=/home/acuenod/scratch/household/01_data/16_public_seqs/long_reads/blast_chr/chr1
chr2_dir=/home/acuenod/scratch/household/01_data/16_public_seqs/long_reads/blast_chr/chr2
outdir=/lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/blast_chr/blastout

for sample in  $(cat /lustre04/scratch/acuenod/household/01_data/16_public_seqs/long_reads/blast_chr/assemblies_2chr.txt)
do
  echo "$sample"
  rm "$outdir"/chr1_db*
  makeblastdb -in "$chr1_dir"/"$sample"_chr_1.fasta  -dbtype nucl -out "$outdir"/chr1_db
  blastn -query "$chr2_dir"/"$sample"_chr_2.fasta -db  "$outdir"/chr1_db \
  -outfmt '6 qseqid sseqid bitscore pident nident mismatch length qcovs qlen qstart qend slen sstart send qseq sseq' \
  -max_target_seqs 5000 -out "$outdir"/"$sample"_chr_blastout.tab
done

rm "$outdir"/chr1_db*

```
